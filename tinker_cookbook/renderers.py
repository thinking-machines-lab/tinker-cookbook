"""
Use viz_sft_dataset to visualize the output of different renderers. E.g.,
    python -m tinker_cookbook.supervised.viz_sft_dataset dataset_path=Tulu3Builder renderer_name=role_colon
"""

import io
import json
import logging
import re
import urllib.request
from datetime import datetime
from enum import StrEnum
from typing import Literal, NotRequired, Optional, Protocol, TypedDict, cast

import pydantic
import tinker
import torch
from PIL import Image

from tinker_cookbook.image_processing_utils import ImageProcessor
from tinker_cookbook.tokenizer_utils import Tokenizer

logger = logging.getLogger(__name__)

# Tool types are based on kosong (https://github.com/MoonshotAI/kosong).


class StrictBase(pydantic.BaseModel):
    """
    Pydantic base class that's immutable and doesn't silently ignore extra fields.
    """

    model_config = pydantic.ConfigDict(frozen=True, extra="forbid")

    def __str__(self) -> str:
        return repr(self)


class ToolCall(StrictBase):
    """
    Structured tool invocation following OpenAI/kosong format.

    This represents a request to invoke a tool/function. The structure follows
    the OpenAI function calling format for compatibility with various LLM APIs.

    Example:
        tool_call = ToolCall(
            function=ToolCall.FunctionBody(
                name="search",
                arguments='{"query_list": ["python async", "pydantic validation"]}'
            ),
            id="call_abc123"
        )
    """

    class FunctionBody(pydantic.BaseModel):
        """
        Tool call function body containing the tool name and arguments.

        The arguments field must be a valid JSON string that will be parsed
        by the tool implementation.
        """

        name: str
        """The name of the tool to be called."""
        arguments: str
        """Arguments of the tool call in JSON string format."""

    type: Literal["function"] = "function"
    """Tool call type, must be 'function' for compatibility."""

    id: str | None = None
    """Optional unique identifier for tracking this specific tool call."""

    function: FunctionBody
    """The function body containing tool name and arguments."""


class UnparsedToolCall(StrictBase):
    """
    Represents a tool call that failed to parse from model output.

    When a model generates text that looks like a tool call but cannot be
    parsed (e.g., invalid JSON), this class captures the raw text and error
    for debugging and optional re-rendering.

    Example:
        unparsed = UnparsedToolCall(
            raw_text='<tool_call>{"name": "search", invalid json}</tool_call>',
            error="Invalid JSON: Expecting property name"
        )
    """

    raw_text: str
    """The original text from the model that failed to parse."""

    error: str
    """Description of what went wrong during parsing."""


class TextPart(TypedDict):
    """
    Container for a text part in a multimodal message.

    Args:

    type: Literal['text']
        The type of the content part, which must be text in this case.
    text: str
        The string content of the content part.
    """

    type: Literal["text"]
    text: str


class ImagePart(TypedDict):
    """
    Container for an image part in a multimodal message.

    Args:

    type: Literal['image']
        The type of the content part, which must be image in this case.
    image: str | Image.Image
        Either a url, data URL, or PIL image.
    """

    type: Literal["image"]
    image: str | Image.Image


# Container for a part of a multimodal message content
ContentPart = TextPart | ImagePart


# NOTE: we use a broad type definition for the role to be flexible
# Common roles are "user", "assistant", "system", "tool"
Role = str

# Content is a string or a list of parts
Content = str | list[ContentPart]


class Message(TypedDict):
    """
    Container for a single turn in a multi-turn conversation.

    Args:

    role: Role
        String that denotes the source of the message, typically system, user, assistant, and tool.
    content: Content
        Content of the message, can be a string, or a list of ContentPart.
    tool_calls: NotRequired[list[ToolCall]]
        Optional sequence of successfully parsed tool calls generated by the model.
    unparsed_tool_calls: NotRequired[list[UnparsedToolCall]]
        Optional sequence of tool calls that failed to parse (e.g., invalid JSON).
        The raw text is preserved for debugging or re-rendering.
    thinking: NotRequired[str]
        Optional thinking produced by the model before its final response.
    trainable: NotRequired[bool]
        Optional indicator whether this message should contribute to the training loss.

    """

    role: Role
    content: Content

    tool_calls: NotRequired[list[ToolCall]]
    unparsed_tool_calls: NotRequired[list["UnparsedToolCall"]]
    thinking: NotRequired[str]
    trainable: NotRequired[bool]
    tool_call_id: NotRequired[str]
    name: NotRequired[str]


class ToolSpec(TypedDict):
    """
    Tool specification following the OpenAI function calling format.

    This represents a tool that can be called by the model, including its name,
    description, and parameter schema.

    Example:
        tool_spec: ToolSpec = {
            "name": "get_weather",
            "description": "Get the current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {"type": "string", "description": "City name"},
                },
                "required": ["location"],
            },
        }
    """

    name: str
    """The name of the tool."""
    description: str
    """A description of what the tool does."""
    parameters: dict
    """JSON Schema object describing the tool's parameters."""


def ensure_text(content: Content) -> str:
    """
    Assert that content is text-only and return it as a string.

    Raises ValueError if content contains images or multiple parts.
    Use this to validate that message content is text-only before
    processing it in code paths that don't support multimodal content.
    """
    if isinstance(content, str):
        return content
    if len(content) == 1 and content[0]["type"] == "text":
        return content[0]["text"]
    raise ValueError(f"Expected text content, got multimodal content with {len(content)} parts")


def _tool_call_payload(tool_call: ToolCall) -> dict[str, object]:
    """Minimal JSON payload for embedding in <tool_call> blocks."""
    # Convert from nested structure to flat format for compatibility
    return {
        "name": tool_call.function.name,
        "arguments": json.loads(tool_call.function.arguments),
    }


class RenderedMessage(TypedDict):
    """
    Container for parts of a rendered message, for masking.

    Args:

    prefix: NotRequired[tinker.EncodedTextChunk]
        Message header that typically includes the speaker's role in the conversation.
    content: list[tinker.ModelInputChunk]
        Inner parts of the message that may include spans of image and text.
    suffix: NotRequired[tinker.EncodedTextChunk]
        Message header that typically includes the turn stop token.

    """

    prefix: NotRequired[tinker.EncodedTextChunk]
    content: list[tinker.ModelInputChunk]
    suffix: NotRequired[tinker.EncodedTextChunk]


class TrainOnWhat(StrEnum):
    LAST_ASSISTANT_MESSAGE = "last_assistant_message"
    ALL_ASSISTANT_MESSAGES = "all_assistant_messages"
    ALL_MESSAGES = "all_messages"
    ALL_TOKENS = "all_tokens"
    ALL_USER_AND_SYSTEM_MESSAGES = "all_user_and_system_messages"
    CUSTOMIZED = "customized"


class Renderer(Protocol):
    """
    Render a message list into training and sampling prompts for language models.
    """

    tokenizer: Tokenizer

    def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer

    def _preprocess_message_parts(self, message: Message) -> list[ImagePart | TextPart]:
        return (
            message["content"]
            if isinstance(message["content"], list)
            else [TextPart(type="text", text=message["content"])]
        )

    @property
    def _bos_tokens(self) -> list[int]:
        return []

    def get_stop_sequences(self) -> list[str] | list[int]:
        raise NotImplementedError

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        raise NotImplementedError

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        raise NotImplementedError

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        """Create message(s) with tool specifications to prepend to conversations.

        Returns one or more messages to prepend to the conversation. This is the
        standard way to add tools - the returned messages should be placed at the
        start of your message list before user/assistant messages.

        Args:
            tools: List of tool specifications.
            system_prompt: The system prompt content.

        Returns:
            List of messages to prepend to the conversation.

        Raises:
            NotImplementedError: If the renderer doesn't support tool calling.
        """
        raise NotImplementedError

    def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> tinker.ModelInput:
        """
        Generates tokens for sampling from the model.

        Args:
            messages: a list of messages to render.
            role: the role of the partial message to be completed.
            prefill: an optional string to prefill in the model's generation.
        """

        chunks: list[tinker.types.ModelInputChunk] = []
        if self._bos_tokens:
            chunks.append(tinker.types.EncodedTextChunk(tokens=self._bos_tokens))
        for idx, message in enumerate(messages):
            rendered_message = self.render_message(idx, message)
            ob_chunk = rendered_message.get("prefix")
            action_chunks = rendered_message["content"]
            if ob_chunk:
                chunks.append(ob_chunk)
            chunks.extend([x for x in action_chunks if x])
        new_partial_message = Message(role=role, content="")
        rendered_message = self.render_message(len(messages), new_partial_message)
        ob_chunk = rendered_message.get("prefix")
        if ob_chunk:
            chunks.append(ob_chunk)
        if prefill:
            chunks.append(
                tinker.types.EncodedTextChunk(
                    tokens=self.tokenizer.encode(prefill, add_special_tokens=False)
                )
            )
        return tinker.ModelInput(chunks=chunks)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[tinker.ModelInput, torch.Tensor]:
        """
        Generates tokens and weights (for SFT) in the most standard way; by concatenating
        together tokens and weights for each message.

        Args:
            messages: a list of messages to render.
            train_on_what: an enum that controls how the weights are assigned to the tokens.
                - TrainOnWhat.LAST_ASSISTANT_MESSAGE: only the last assistant message is used for training
                - TrainOnWhat.ALL_ASSISTANT_MESSAGES: all assistant messages are used for training
                - TrainOnWhat.ALL_MESSAGES: all messages are used for training
                - TrainOnWhat.ALL_TOKENS: all tokens are used for training
                - TrainOnWhat.ALL_USER_AND_SYSTEM_MESSAGES: all user and system messages are used for training
                - TrainOnWhat.CUSTOMIZED: each message has a trainable field, and the weights are assigned based on the trainable field

        Returns:
            A tuple of two tensors:
                - model_input: the tinker ModelInput for your model
                - weights: a tensor of weights
        """

        model_input_chunks_weights: list[tuple[tinker.types.ModelInputChunk, float]] = []
        if self._bos_tokens:
            model_input_chunks_weights.append(
                (tinker.types.EncodedTextChunk(tokens=self._bos_tokens), 0.0)
            )

        for idx, message in enumerate(messages):
            if train_on_what == TrainOnWhat.CUSTOMIZED:
                assert "trainable" in message, (
                    "When using CUSTOMIZED train_on_what, each message must have a trainable field: True if loss is applied on this message, False otherwise"
                )
            else:
                assert "trainable" not in message, (
                    "When using non-CUSTOMIZED train_on_what, each message must not have a trainable field. Either change train_on_what to CUSTOMIZED or remove the trainable field from the message"
                )

            is_last_message = idx == len(messages) - 1
            is_assistant = message["role"] == "assistant"
            is_user_or_system = message["role"] in ["user", "system"]

            # only apply weight to observation part if train_on_what is ALL_TOKENS
            rendered_message = self.render_message(idx, message, is_last=is_last_message)
            ob_part = rendered_message.get("prefix")
            action_parts = rendered_message.get("content")
            action_tail = rendered_message.get("suffix")

            ob_weight = int(train_on_what == TrainOnWhat.ALL_TOKENS)
            if ob_part:
                model_input_chunks_weights += [(ob_part, ob_weight)]

            match train_on_what:
                case TrainOnWhat.LAST_ASSISTANT_MESSAGE:
                    action_has_weight = is_last_message and is_assistant
                case TrainOnWhat.ALL_ASSISTANT_MESSAGES:
                    action_has_weight = is_assistant
                case TrainOnWhat.ALL_MESSAGES:
                    action_has_weight = True
                case TrainOnWhat.ALL_TOKENS:
                    action_has_weight = True
                case TrainOnWhat.ALL_USER_AND_SYSTEM_MESSAGES:
                    action_has_weight = is_user_or_system
                case TrainOnWhat.CUSTOMIZED:
                    action_has_weight = message.get("trainable", False)
                case _:
                    raise ValueError(f"Unknown train_on_what: {train_on_what}")

            model_input_chunks_weights += [
                (action_part, int(action_has_weight)) for action_part in action_parts if action_part
            ]

            # action tail is effectively the stop_token and the start token for the next turn
            # e.g. \n\nUser:
            if is_last_message and action_tail:
                model_input_chunks_weights += [(action_tail, int(action_has_weight))]

        weights_data = [w for chunk, w in model_input_chunks_weights for _ in range(chunk.length)]
        weights_tensor = torch.tensor(weights_data)

        model_input_chunks = [chunk for chunk, _ in model_input_chunks_weights]
        return tinker.ModelInput(chunks=model_input_chunks), weights_tensor


def tokens_weights_from_strings_weights(
    strings_weights: list[tuple[str, float]],
    tokenizer: Tokenizer,
) -> tuple[torch.Tensor, torch.Tensor]:
    strings, weights = zip(*strings_weights, strict=True)
    token_chunks = [tokenizer.encode(s, add_special_tokens=i == 0) for i, s in enumerate(strings)]
    weights = torch.cat(
        [torch.full((len(chunk),), w) for chunk, w in zip(token_chunks, weights, strict=True)]
    )
    tokens = torch.cat([torch.tensor(chunk) for chunk in token_chunks])
    assert tokens.dtype == torch.int64
    return tokens, weights


def parse_response_for_stop_token(
    response: list[int], tokenizer: Tokenizer, stop_token: int
) -> tuple[Message, bool]:
    """Parse response for a single stop token.

    We expect a properly rendered response to have exactly one stop token; but it may have zero if e.g. the model
    ran out of tokens when sampling, which will incur a format error. If there are > 1, there is likely a bug in the
    sampler and we should error.
    """
    emt_count = response.count(stop_token)
    if emt_count == 0:
        str_response = tokenizer.decode(response)
        logger.debug(f"Response is not a valid assistant response: {str_response}")
        return Message(role="assistant", content=str_response), False
    elif emt_count == 1:
        str_response = tokenizer.decode(response[: response.index(stop_token)])
        return Message(role="assistant", content=str_response), True
    else:
        raise ValueError(
            f"When parsing response, expected to split into 1 or 2 pieces using stop tokens, but got {emt_count}. "
            "You probably are using the wrong stop tokens when sampling"
        )


class RoleColonRenderer(Renderer):
    """
    format like this:
        User: <content>

        Assistant: <content>

    This is basically the format used by DeepSeek, and similar to the format used by Anthropic,
    except that they use "Human" instead of "User".
    """

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "Thinking tokens not supported in RoleColonRenderer"
        assert isinstance(message["content"], str), (
            "RoleColonRenderer only supports message with string content"
        )
        ob_str = message["role"].capitalize() + ":"
        # Observation (prompt) part
        ac_str = " " + message["content"] + "\n\n"
        # Action part
        ac_tail_str = "User:" if message["role"] == "assistant" else "<UNUSED>"
        # Action part that's only included in the last message in SFT
        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(
                tokens=self.tokenizer.encode(ac_str, add_special_tokens=False)
            )
        ]
        suffix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ac_tail_str, add_special_tokens=False)
        )
        return RenderedMessage(prefix=prefix, content=content, suffix=suffix)

    def get_stop_sequences(self) -> list[str]:
        return ["\n\nUser:"]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        str_response = self.tokenizer.decode(response)
        splitted = str_response.split("\n\nUser:")
        if len(splitted) == 1:
            logger.debug(f"Response is not a valid assistant response: {str_response}")
            return Message(role="assistant", content=str_response.strip()), False
        elif len(splitted) == 2:
            before, _after = splitted
            return Message(role="assistant", content=before.strip()), True
        else:
            raise ValueError(
                f"When parsing response, expected to split into 1 or 2 pieces using stop tokens, but got {len(splitted)}. "
                "You probably are using the wrong stop tokens when sampling"
            )

    @property
    def _bos_tokens(self) -> list[int]:
        bos_token_str = self.tokenizer.bos_token
        if bos_token_str is None:
            return []
        assert isinstance(bos_token_str, str)
        return self.tokenizer.encode(bos_token_str, add_special_tokens=False)

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        raise NotImplementedError("RoleColonRenderer does not support tool calling")


class Llama3Renderer(Renderer):
    """
    Format like this:
        <|begin_of_text|><|start_header_id|>system<|end_header_id|>

        You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>

        What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

    Note: The HF template prepends "Cutting Knowledge Date: December 2023\\nToday Date: {date}"
    to system messages. We chose not to do this because it seemed janky. If you want to match
    the HF template exactly, modify render_message to prepend this info for system messages.
    """

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "CoT tokens not supported in Llama3"
        assert isinstance(message["content"], str), (
            "Llama3Renderer only supports message with string content"
        )

        # Determine role for header
        # Tool responses use "ipython" role in Llama 3 format
        role = message["role"]
        if role == "tool":
            role = "ipython"

        ob_str = f"<|start_header_id|>{role}<|end_header_id|>\n\n"

        # Build action content
        ac_str = message["content"]

        # Handle tool calls in assistant messages
        # Llama 3 format: <function=function_name>{"arg": "value"}</function>
        if "tool_calls" in message and message["tool_calls"]:
            tool_call_strs = []
            for tool_call in message["tool_calls"]:
                func_name = tool_call.function.name
                args = tool_call.function.arguments
                tool_call_strs.append(f"<function={func_name}>{args}</function>")
            ac_str += "".join(tool_call_strs)

        ac_str += "<|eot_id|>"

        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(
                tokens=self.tokenizer.encode(ac_str, add_special_tokens=False)
            )
        ]
        return RenderedMessage(prefix=prefix, content=content)

    @property
    def _bos_tokens(self) -> list[int]:
        return self.tokenizer.encode("<|begin_of_text|>", add_special_tokens=False)

    @property
    def _end_message_token(self) -> int:
        (token,) = self.tokenizer.encode("<|eot_id|>", add_special_tokens=False)
        return token

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def _parse_llama_tool_calls(
        self, content: str
    ) -> tuple[list[ToolCall], list[UnparsedToolCall]]:
        """Parse tool calls from Llama 3 format.

        Llama 3 uses: <function=function_name>{"arg": "value"}</function>

        Returns:
            Tuple of (successfully parsed tool calls, failed parses).
        """
        tool_calls: list[ToolCall] = []
        unparsed_tool_calls: list[UnparsedToolCall] = []

        for match in re.finditer(
            r"<function=(\w+)>(.*?)</function>",
            content,
            re.DOTALL,
        ):
            raw_text = match.group(0)
            func_name = match.group(1)
            args_str = match.group(2).strip()
            try:
                # Validate JSON
                json.loads(args_str)
                tool_calls.append(
                    ToolCall(
                        function=ToolCall.FunctionBody(name=func_name, arguments=args_str),
                    )
                )
            except json.JSONDecodeError as e:
                unparsed_tool_calls.append(
                    UnparsedToolCall(raw_text=raw_text, error=f"Invalid JSON: {e}")
                )

        return tool_calls, unparsed_tool_calls

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        assistant_message, parse_success = parse_response_for_stop_token(
            response, self.tokenizer, self._end_message_token
        )
        if not parse_success:
            return assistant_message, False

        assert isinstance(assistant_message["content"], str)
        content = assistant_message["content"]

        # Parse tool calls
        tool_calls, unparsed_tool_calls = self._parse_llama_tool_calls(content)
        if tool_calls:
            assistant_message["tool_calls"] = tool_calls
        if unparsed_tool_calls:
            assistant_message["unparsed_tool_calls"] = unparsed_tool_calls

        # Strip all function blocks from content (both parsed and unparsed)
        if tool_calls or unparsed_tool_calls:
            content = re.sub(
                r"\s*<function=\w+>.*?</function>",
                "",
                content,
                flags=re.DOTALL,
            )
            assistant_message["content"] = content.strip()

        return assistant_message, True

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        """Create system message with Llama 3 tool specifications.

        Llama 3.1 supports two tool calling formats. We use the function-tag format
        which works for custom tools without special environment setup.

        Reference: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/prompt_format.md
        """
        tools_text = ""
        if tools:
            tool_lines = "\n\n".join(json.dumps(tool, indent=2) for tool in tools)
            tools_text = f"""You have access to the following functions:

{tool_lines}

If you choose to call a function, ONLY reply in the following format with no prefix or suffix:

<function=example_function_name>{{"example_name": "example_value"}}</function>

Reminder:
- Function calls MUST follow the specified format, start with <function= and end with </function>
- Required parameters MUST be specified
- Only call one function at a time
- Put the entire function call reply on one line

"""

        return [Message(role="system", content=tools_text + system_prompt)]


class Qwen3Renderer(Renderer):
    """
    Renderer for Qwen3 models with thinking enabled.

    This renderer is designed to match HuggingFace's Qwen3 chat template behavior
    (with enable_thinking=True, which is the default). This ensures compatibility
    with the OpenAI-compatible /chat/completions endpoint, which uses HF templates.

    Reference: https://huggingface.co/Qwen/Qwen3-8B/blob/main/tokenizer_config.json

    Format:
        <|im_start|>system
        You are Qwen, created by Alibaba Cloud.<|im_end|>
        <|im_start|>user
        What can you help me with?<|im_end|>
        <|im_start|>assistant
        <think>
        [reasoning content]
        </think>
        I can help you with...<|im_end|>

    The default strip_thinking_from_history=True matches HF behavior where thinking
    blocks are stripped from historical assistant messages in multi-turn conversations.
    Use strip_thinking_from_history=False for multi-turn RL to get the extension property.
    """

    def __init__(self, tokenizer: Tokenizer, strip_thinking_from_history: bool = True):
        """
        Args:
            tokenizer: The tokenizer to use for encoding.
            strip_thinking_from_history: When True (default), strips <think>...</think> blocks
                from assistant messages in multi-turn history. This matches HuggingFace's
                Qwen3 chat template behavior. Set to False to preserve thinking in history
                (useful for multi-turn RL where you need the extension property).

        Note: When strip_thinking_from_history=True, this renderer produces identical
        tokens to HuggingFace's apply_chat_template with enable_thinking=True.

        See /rl/sequence-extension in the docs for details on how strip_thinking_from_history
        affects multi-turn RL compute efficiency.
        """
        super().__init__(tokenizer)
        self.strip_thinking_from_history = strip_thinking_from_history

    def _get_qwen_role_for_message(self, message: Message) -> str:
        """Get the role to use for rendering a message in Qwen format.

        Per HuggingFace Qwen3 chat template, tool messages are rendered with role "user".
        """
        role = message["role"]
        if role == "tool":
            return "user"
        return role

    def _wrap_qwen_tool_response(self, content: str) -> str:
        """Wrap tool response content in Qwen's <tool_response> tags."""
        return f"<tool_response>\n{content}\n</tool_response>"

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "TODO: support CoT in Qwen3 renderer"
        assert isinstance(message["content"], str), (
            "Qwen3Renderer only supports message with string content"
        )
        maybe_newline = "\n" if idx > 0 else ""

        role = self._get_qwen_role_for_message(message)
        ob_str = f"{maybe_newline}<|im_start|>{role}\n"
        ac_content = message["content"]

        # Handle tool response wrapping
        if message["role"] == "tool":
            ac_content = self._wrap_qwen_tool_response(ac_content)

        if (
            self.strip_thinking_from_history
            and message["role"] == "assistant"
            and "</think>" in ac_content
            and not is_last
        ):
            # Multi-turn conversation, we remove the thinking section from the assistant message.
            # This matches how Qwen3 models were trained - they only see their own thinking
            # during the current turn, not from previous turns.
            ac_content = ac_content.split("</think>")[1].lstrip()
        elif message["role"] == "assistant" and "<think>" not in ac_content and is_last:
            # Matching the paper, we force the assistant to start with <think>. Some SFT datasets include
            # <think> in the assistant messages, we so don't need to re-add it in those cases.
            ob_str += "<think>\n"
        # Observation (prompt) part
        if "tool_calls" in message:
            # Add leading newline to match HF template behavior
            ac_content += "\n" + "\n".join(
                [
                    f"<tool_call>\n{json.dumps(_tool_call_payload(tool_call))}\n</tool_call>"
                    for tool_call in message["tool_calls"]
                ]
            )
        ac_content += "<|im_end|>"
        # Action part
        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(
                tokens=self.tokenizer.encode(ac_content, add_special_tokens=False)
            )
        ]
        return RenderedMessage(prefix=prefix, content=content)

    @property
    def _end_message_token(self) -> int:
        tokens = self.tokenizer.encode("<|im_end|>", add_special_tokens=False)
        assert len(tokens) == 1, f"Expected single token for <|im_end|>, got {len(tokens)}"
        return tokens[0]

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def _parse_single_tool_call(
        self, tool_call_str: str, raw_text: str
    ) -> ToolCall | UnparsedToolCall:
        """Parse a single tool call JSON string into a ToolCall object.

        Args:
            tool_call_str: The JSON content inside the tool_call tags.
            raw_text: The full raw text including tags (for error reporting).

        Returns:
            ToolCall on success, UnparsedToolCall on failure.
        """
        try:
            tool_call = json.loads(tool_call_str.strip())
        except json.JSONDecodeError as e:
            return UnparsedToolCall(raw_text=raw_text, error=f"Invalid JSON: {e}")

        if not isinstance(tool_call, dict):
            return UnparsedToolCall(raw_text=raw_text, error="Tool call is not a JSON object")

        name = tool_call.get("name")
        arguments = tool_call.get("arguments")
        tool_id = tool_call.get("id")

        if not isinstance(name, str):
            return UnparsedToolCall(raw_text=raw_text, error="Missing or invalid 'name' field")
        if not isinstance(arguments, dict):
            return UnparsedToolCall(raw_text=raw_text, error="Missing or invalid 'arguments' field")

        if tool_id is not None and not isinstance(tool_id, str):
            tool_id = None

        return ToolCall(
            function=ToolCall.FunctionBody(name=name, arguments=json.dumps(arguments)),
            id=tool_id,
        )

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        assistant_message, parse_success = parse_response_for_stop_token(
            response, self.tokenizer, self._end_message_token
        )
        if not parse_success:
            return assistant_message, False

        # Follow Qwen docs and Qwen-Agent's tool calling prompt to use <tool_call>...</tool_call> tags to wrap the tool call.
        # - https://qwen.readthedocs.io/en/latest/getting_started/concepts.html#tool-calling
        # - https://github.com/QwenLM/Qwen-Agent/blob/main/qwen_agent/llm/fncall_prompts/nous_fncall_prompt.py#L279-L282
        assert isinstance(assistant_message["content"], str)
        content = assistant_message["content"]

        # Find all tool calls in the response
        tool_calls: list[ToolCall] = []
        unparsed_tool_calls: list[UnparsedToolCall] = []

        for match in re.finditer(r"<tool_call>(.*?)</tool_call>", content, re.DOTALL):
            raw_text = match.group(0)  # Full match including tags
            parsed = self._parse_single_tool_call(match.group(1), raw_text)
            if isinstance(parsed, UnparsedToolCall):
                unparsed_tool_calls.append(parsed)
            else:
                tool_calls.append(parsed)

        if tool_calls:
            assistant_message["tool_calls"] = tool_calls
        if unparsed_tool_calls:
            assistant_message["unparsed_tool_calls"] = unparsed_tool_calls

        # Strip all tool_call blocks from content (both parsed and unparsed)
        if tool_calls or unparsed_tool_calls:
            content = re.sub(r"\n?<tool_call>.*?</tool_call>", "", content, flags=re.DOTALL)
            assistant_message["content"] = content.strip()

        return assistant_message, True

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        """Create system message with Qwen3 tool specifications.

        Qwen3 uses XML `<tools>` tags with JSON tool definitions appended to the
        system message content.

        Reference: https://huggingface.co/Qwen/Qwen3-8B/blob/main/tokenizer_config.json
        """
        tools_text = ""
        if tools:
            tool_lines = "\n".join(json.dumps(tool, separators=(",", ":")) for tool in tools)
            tools_text = f"""

# Tools

You may call one or more functions to assist with the user query.

You are provided with function signatures within <tools></tools> XML tags:
<tools>
{tool_lines}
</tools>

For each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:
<tool_call>
{{"name": <function-name>, "arguments": <args-json-object>}}
</tool_call>"""

        return [Message(role="system", content=system_prompt + tools_text)]


class Qwen3DisableThinkingRenderer(Qwen3Renderer):
    """
    Renderer for Qwen3 hybrid models with thinking disabled.

    This renderer matches HuggingFace's Qwen3 chat template behavior with
    enable_thinking=False (or thinking=False for apply_chat_template). It adds
    empty <think>\\n\\n</think>\\n\\n blocks to assistant messages, signaling to
    the model that it should respond directly without extended reasoning.

    Use this renderer when you want to train or sample from Qwen3 models in
    "non-thinking" mode while maintaining compatibility with the OpenAI endpoint.
    """

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        # Add empty thinking block to assistant messages if not already present
        if message["role"] == "assistant":
            content = message.get("content", "")
            assert isinstance(content, str), (
                "Qwen3DisableThinkingRenderer only supports message with string content"
            )
            if "<think>" not in content:
                message = message.copy()
                message["content"] = "<think>\n\n</think>\n\n" + content
        return super().render_message(idx, message, is_last=is_last)

    def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> tinker.ModelInput:
        prefill = "<think>\n\n</think>\n\n" + (prefill or "")
        return super().build_generation_prompt(messages, role, prefill)


class Qwen3InstructRenderer(Qwen3Renderer):
    """
    Renderer for Qwen3 instruct 2507 models. Unlike the earlier Qwen3 models, these models do not
    use the <think> tag at all.
    """

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "CoT tokens not supported in Qwen3 instruct 2507"
        assert isinstance(message["content"], str), (
            "Qwen3InstructRenderer only supports message with string content"
        )
        maybe_newline = "\n" if idx > 0 else ""

        role = self._get_qwen_role_for_message(message)
        ob_str = f"{maybe_newline}<|im_start|>{role}\n"
        ac_content = message["content"]

        # Handle tool response wrapping
        if message["role"] == "tool":
            ac_content = self._wrap_qwen_tool_response(ac_content)

        # Observation (prompt) part
        if "tool_calls" in message:
            # Add leading newline to match HF template behavior
            ac_content += "\n" + "\n".join(
                [
                    f"<tool_call>\n{json.dumps(_tool_call_payload(tool_call))}\n</tool_call>"
                    for tool_call in message["tool_calls"]
                ]
            )
        ac_content += "<|im_end|>"
        # Action part
        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(
                tokens=self.tokenizer.encode(ac_content, add_special_tokens=False)
            )
        ]
        return RenderedMessage(prefix=prefix, content=content)


class ImageProcessorProtocol(Protocol):
    merge_size: int
    patch_size: int

    def get_number_of_image_patches(
        self, height: int, width: int, images_kwargs: Optional[dict] = None
    ) -> int:
        raise NotImplementedError()


def image_to_chunk(
    image_or_str: Image.Image | str, image_processor: ImageProcessorProtocol
) -> tinker.types.ImageChunk:
    """
    Convert a PIL Image to a tinker.types.ImageChunk for QwenVL
    """

    # load an image from a data URI or a URL
    if isinstance(image_or_str, str):
        with urllib.request.urlopen(image_or_str) as response:
            pil_image = Image.open(io.BytesIO(response.read()))

    # Otherwise the image is a PIL image and can be loaded directly
    elif isinstance(image_or_str, Image.Image):
        pil_image = image_or_str

    # Validate the provided data is actually a valid image type
    else:
        raise ValueError("The provided image must be a PIL.Image.Image, URL, or data URI.")

    # Convert to RGB if needed (JPEG doesn't support RGBA/LA/P modes)
    if pil_image.mode in ("RGBA", "LA", "P"):
        pil_image = pil_image.convert("RGB")

    img_byte_arr = io.BytesIO()
    pil_image.save(img_byte_arr, format="JPEG")
    image_data = img_byte_arr.getvalue()

    width, height = pil_image.size
    num_image_tokens = (
        image_processor.get_number_of_image_patches(height, width, images_kwargs={})
        // image_processor.merge_size**2
    )

    return tinker.types.ImageChunk(
        data=image_data,
        format="jpeg",
        expected_tokens=num_image_tokens,
    )


class Qwen3VLRenderer(Qwen3Renderer):
    """
    Format like this:
        <|im_start|>system
        You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
        <|im_start|>user
        What can you help me with?<|im_end|>
        <|im_start|>assistant
        <think>

        </think>
        I can help you with...<|im_end|>

    It is currently missing Qwen 3's functionality for removing thinking spans in multi-turn conversations.
    """

    image_processor: ImageProcessor

    def __init__(self, tokenizer: Tokenizer, image_processor: ImageProcessor):
        self.tokenizer = tokenizer
        self.image_processor = image_processor

    def _preprocess_message_parts(self, message: Message) -> list[ImagePart | TextPart]:
        chunks: list[ImagePart | TextPart] = []

        for content_chunk in super()._preprocess_message_parts(message):
            if content_chunk["type"] == "image":
                chunks.append(TextPart(type="text", text="<|vision_start|>"))

            chunks.append(content_chunk)

            if content_chunk["type"] == "image":
                chunks.append(TextPart(type="text", text="<|vision_end|>"))

        return chunks

    def _wrap_qwen_tool_response_chunks(
        self, chunks: list[ImagePart | TextPart]
    ) -> list[ImagePart | TextPart]:
        """Wrap content chunks in Qwen's <tool_response> tags for multimodal messages."""
        return (
            [TextPart(type="text", text="<tool_response>\n")]
            + chunks
            + [TextPart(type="text", text="\n</tool_response>")]
        )

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "TODO: support CoT in Qwen3 renderer"
        maybe_newline = "\n" if idx > 0 else ""

        role = self._get_qwen_role_for_message(message)
        ob_str = f"{maybe_newline}<|im_start|>{role}\n"

        ac_content_chunks = self._preprocess_message_parts(message)

        # Handle tool response wrapping
        if message["role"] == "tool":
            ac_content_chunks = self._wrap_qwen_tool_response_chunks(ac_content_chunks)

        contains_think_token = any(
            [
                (
                    "<think>" in x
                    if isinstance(x, str)
                    else "<think>" in x["text"]
                    if isinstance(x, dict) and x["type"] == "text"
                    else False
                )
                for x in ac_content_chunks
            ]
        )
        if message["role"] == "assistant" and not contains_think_token:
            # Matching the paper, we force the assistant to start with <think>. Some SFT datasets include
            # <think> in the assistant messages, we so don't need to re-add it in those cases.
            ob_str += "<think>\n"
        # Observation (prompt) part
        if "tool_calls" in message:
            ac_content_chunks += [
                TextPart(
                    type="text",
                    text="\n".join(
                        [
                            f"<tool_call>\n{json.dumps(_tool_call_payload(tool_call))}\n</tool_call>"
                            for tool_call in message["tool_calls"]
                        ]
                    ),
                )
            ]
        ac_content_chunks += [TextPart(type="text", text="<|im_end|>")]
        # Action part

        ac_content_chunks_encoded: list[tinker.ModelInputChunk] = [
            image_to_chunk(
                image_or_str=x["image"],
                image_processor=cast(ImageProcessorProtocol, self.image_processor),
            )
            if x["type"] == "image"
            else tinker.EncodedTextChunk(
                tokens=self.tokenizer.encode(x["text"], add_special_tokens=False)
            )
            for x in ac_content_chunks
        ]

        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        return RenderedMessage(prefix=prefix, content=ac_content_chunks_encoded)


class Qwen3VLInstructRenderer(Qwen3VLRenderer):
    """
    Renderer for Qwen3-VL Instruct models.

    Unlike the Qwen3-VL Thinking models, The Qwen3-VL Instruct models do not use the <think> tag.
    """

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "CoT tokens not supported in Qwen3-VL instruct"
        maybe_newline = "\n" if idx > 0 else ""

        role = self._get_qwen_role_for_message(message)
        ob_str = f"{maybe_newline}<|im_start|>{role}\n"

        ac_content_chunks = self._preprocess_message_parts(message)

        # Handle tool response wrapping
        if message["role"] == "tool":
            ac_content_chunks = self._wrap_qwen_tool_response_chunks(ac_content_chunks)

        if "tool_calls" in message:
            ac_content_chunks += [
                TextPart(
                    type="text",
                    text="\n".join(
                        [
                            f"<tool_call>\n{json.dumps(_tool_call_payload(tool_call))}\n</tool_call>"
                            for tool_call in message["tool_calls"]
                        ]
                    ),
                )
            ]
        ac_content_chunks += [TextPart(type="text", text="<|im_end|>")]

        ac_content_chunks_encoded: list[tinker.ModelInputChunk] = [
            image_to_chunk(
                image_or_str=x["image"],
                image_processor=cast(ImageProcessorProtocol, self.image_processor),
            )
            if x["type"] == "image"
            else tinker.EncodedTextChunk(
                tokens=self.tokenizer.encode(x["text"], add_special_tokens=False)
            )
            for x in ac_content_chunks
        ]

        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        return RenderedMessage(prefix=prefix, content=ac_content_chunks_encoded)


class DeepSeekV3Renderer(Renderer):
    """
    Format like this (no newlines between messages):
        <|begin_of_sentence|><|User|>What can you help me with?<|Assistant|><think>Thinking...</think>I can help you with...<|end_of_sentence|>
    For no-think, just use <|Assistant|></think>

    System messages at position 0 are rendered without role tokens (matching HF template).
    System messages at later positions require system_role_as_user=True to convert to user role.
    """

    def __init__(self, tokenizer: Tokenizer, system_role_as_user: bool = False):
        super().__init__(tokenizer)
        self.system_role_as_user = system_role_as_user

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("thinking") is None, "TODO: support thinking attribute in DSV3 renderer"
        assert isinstance(message["content"], str), (
            "DeepSeekV3Renderer only supports message with string content"
        )
        if message["role"] == "system":
            # HF template collects all system messages at the start without role tokens
            # We only support this for idx=0; later system messages need system_role_as_user=True
            if idx == 0:
                ob = []
                ac_str = message["content"]
            elif self.system_role_as_user:
                # Convert later system messages to user role
                role_token = self._get_special_token("User")
                ob = [role_token]
                ac_str = message["content"]
            else:
                raise ValueError(
                    "DeepSeek only supports system message at start. "
                    "Use system_role_as_user=True to convert later system messages to user role."
                )
        elif message["role"] == "user":
            role_token = self._get_special_token("User")
            ob = [role_token]
            ac_str = message["content"]
        elif message["role"] == "assistant":
            role_token = self._get_special_token("Assistant")
            ob = [role_token]
            # Add </think> after assistant role (no-think mode) to match HF template
            ac_str = "</think>" + message["content"]
        elif message["role"] == "tool":
            # Tool responses use special tool output tokens to match HF template
            ob = self.tokenizer.encode("<｜tool▁output▁begin｜>", add_special_tokens=False)
            ac_str = message["content"] + "<｜tool▁output▁end｜>"
        else:
            raise ValueError(f"Unsupported role: {message['role']}")

        # Handle tool calls in assistant messages
        # HF format: <｜tool▁calls▁begin｜><｜tool▁call▁begin｜>name<｜tool▁sep｜>args<｜tool▁call▁end｜><｜tool▁calls▁end｜>
        if "tool_calls" in message and message["tool_calls"]:
            ac_str += "<｜tool▁calls▁begin｜>"
            for tool_call in message["tool_calls"]:
                func_name = tool_call.function.name
                args = tool_call.function.arguments
                ac_str += f"<｜tool▁call▁begin｜>{func_name}<｜tool▁sep｜>{args}<｜tool▁call▁end｜>"
            ac_str += "<｜tool▁calls▁end｜>"

        ac = self.tokenizer.encode(ac_str, add_special_tokens=False)

        # Add end_of_sentence only for assistant messages with content
        # (not for empty generation prompt messages)
        if message["role"] == "assistant" and message["content"]:
            ac.append(self._end_message_token)

        content: list[tinker.ModelInputChunk] = [tinker.types.EncodedTextChunk(tokens=ac)]
        # Only include prefix if non-empty; tinker rejects empty token chunks with
        # "Chunk N has empty tokens list". This happens for system messages at idx=0.
        if ob:
            return RenderedMessage(prefix=tinker.types.EncodedTextChunk(tokens=ob), content=content)
        else:
            return RenderedMessage(content=content)

    def _get_special_token(self, name: str) -> int:
        sep = chr(65372)
        s = f"<{sep}{name}{sep}>"
        res = self.tokenizer.encode(s, add_special_tokens=False)
        assert len(res) == 1, f"Expected single token for {s}, got {res}"
        return res[0]

    @property
    def _bos_tokens(self) -> list[int]:
        return [self._get_special_token("begin▁of▁sentence")]

    @property
    def _end_message_token(self) -> int:
        return self._get_special_token("end▁of▁sentence")

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> tinker.ModelInput:
        """Override to skip assistant prefix after tool messages (matches HF behavior)."""
        chunks: list[tinker.types.ModelInputChunk] = []
        if self._bos_tokens:
            chunks.append(tinker.types.EncodedTextChunk(tokens=self._bos_tokens))

        for idx, message in enumerate(messages):
            rendered = self.render_message(idx, message)
            prefix = rendered.get("prefix")
            if prefix:
                chunks.append(prefix)
            chunks.extend(rendered["content"])

        # Add assistant prefix only if last message is NOT tool (HF behavior)
        if not messages or messages[-1]["role"] != "tool":
            rendered = self.render_message(len(messages), Message(role=role, content=""))
            prefix = rendered.get("prefix")
            if prefix:
                chunks.append(prefix)
            chunks.extend(rendered["content"])  # includes </think>

        if prefill:
            chunks.append(
                tinker.types.EncodedTextChunk(
                    tokens=self.tokenizer.encode(prefill, add_special_tokens=False)
                )
            )

        return tinker.ModelInput(chunks=chunks)

    def _parse_deepseek_tool_calls(
        self, content: str
    ) -> tuple[list[ToolCall], list[UnparsedToolCall]]:
        """Parse tool calls from DeepSeek V3.1 format.

        Expected format (per HuggingFace model card and chat template):
            <｜tool▁calls▁begin｜><｜tool▁call▁begin｜>func_name<｜tool▁sep｜>{"arg":"value"}<｜tool▁call▁end｜><｜tool▁calls▁end｜>

        Multiple tool calls are chained directly without separators.

        References:
            - DeepSeek V3.1 Model Card: https://huggingface.co/deepseek-ai/DeepSeek-V3.1
            - Chat Template: https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/assets/chat_template.jinja
        """
        tool_calls: list[ToolCall] = []
        unparsed_tool_calls: list[UnparsedToolCall] = []

        calls_match = re.search(
            r"<｜tool▁calls▁begin｜>(.*?)<｜tool▁calls▁end｜>", content, re.DOTALL
        )
        if not calls_match:
            return tool_calls, unparsed_tool_calls

        for match in re.finditer(
            r"<｜tool▁call▁begin｜>(\w+)<｜tool▁sep｜>(.*?)<｜tool▁call▁end｜>",
            calls_match.group(1),
            re.DOTALL,
        ):
            raw_text = match.group(0)
            func_name, args_str = match.group(1), match.group(2).strip()

            try:
                json.loads(args_str)
                tool_calls.append(
                    ToolCall(function=ToolCall.FunctionBody(name=func_name, arguments=args_str))
                )
            except json.JSONDecodeError as e:
                unparsed_tool_calls.append(
                    UnparsedToolCall(raw_text=raw_text, error=f"Invalid JSON: {e}")
                )

        return tool_calls, unparsed_tool_calls

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        assistant_message, parse_success = parse_response_for_stop_token(
            response, self.tokenizer, self._end_message_token
        )
        if not parse_success:
            return assistant_message, False

        assert isinstance(assistant_message["content"], str)
        content = assistant_message["content"]

        # Parse tool calls
        tool_calls, unparsed_tool_calls = self._parse_deepseek_tool_calls(content)
        if tool_calls:
            assistant_message["tool_calls"] = tool_calls
        if unparsed_tool_calls:
            assistant_message["unparsed_tool_calls"] = unparsed_tool_calls

        # Strip tool calls section from content (both parsed and unparsed)
        if tool_calls or unparsed_tool_calls:
            content = re.sub(
                r"\s*<｜tool▁calls▁begin｜>.*?<｜tool▁calls▁end｜>",
                "",
                content,
                flags=re.DOTALL,
            )
            assistant_message["content"] = content.strip()

        return assistant_message, True

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        """Create system message with DeepSeek V3.1 tool specifications.

        DeepSeek V3.1 tool calling requires tools to be described in the system message
        using a specific format with ### headers and inline JSON parameters.

        Note: Tool calling is supported in non-thinking mode only.

        References:
            - DeepSeek V3.1 Model Card (ToolCall section): https://huggingface.co/deepseek-ai/DeepSeek-V3.1
            - DeepSeek V3.1 Chat Template: https://huggingface.co/deepseek-ai/DeepSeek-V3.1/blob/main/assets/chat_template.jinja
            - DeepSeek API Tool Calls Guide: https://api-docs.deepseek.com/guides/tool_calls
        """
        tools_text = ""
        if tools:
            # Format each tool with ### header, description, and parameters
            tool_blocks = []
            for tool in tools:
                tool_block = f"""### {tool["name"]}
Description: {tool["description"]}

Parameters: {json.dumps(tool["parameters"])}"""
                tool_blocks.append(tool_block)

            tools_text = f"""

## Tools
You have access to the following tools:

{chr(10).join(tool_blocks)}

IMPORTANT: ALWAYS adhere to this exact format for tool use:
<｜tool▁calls▁begin｜><｜tool▁call▁begin｜>tool_call_name<｜tool▁sep｜>tool_call_arguments<｜tool▁call▁end｜><｜tool▁calls▁end｜>

Where:
- `tool_call_name` must be an exact match to one of the available tools
- `tool_call_arguments` must be valid JSON that strictly follows the tool's Parameters Schema
- For multiple tool calls, chain them directly without separators or spaces"""

        return [Message(role="system", content=system_prompt + tools_text)]


class DeepSeekV3DisableThinkingRenderer(DeepSeekV3Renderer):
    """
    Renderer that disables thinking for DsV3 models.

    Note: The base DeepSeekV3Renderer handles </think> addition to match HF templates,
    so this renderer just inherits that behavior. This class is kept for backwards
    compatibility and explicit naming.
    """

    pass


class KimiK2Renderer(Renderer):
    """
    Format for moonshotai/Kimi-K2-Thinking:
        <|im_system|>system<|im_middle|>You are Kimi, an AI assistant created by Moonshot AI.<|im_end|>
        <|im_user|>user<|im_middle|>What can you help me with?<|im_end|>
        <|im_assistant|>assistant<|im_middle|><think>reasoning</think>I can help you with...<|im_end|>

    Historical assistant messages use empty <think></think> blocks, while the final assistant
    response preserves reasoning_content in the thinking block.

    Note: Per the HuggingFace chat template, the default system message is automatically
    prepended if no system message is provided. This ensures train-eval consistency when
    using HF's apply_chat_template for inference.
    """

    DEFAULT_SYSTEM_PROMPT = "You are Kimi, an AI assistant created by Moonshot AI."

    def _ensure_system_message(self, messages: list[Message]) -> list[Message]:
        """Prepend default system message if no system message is present.

        This matches the HuggingFace chat template behavior where a default system
        message is automatically added when none is provided.
        """
        if not messages or messages[0]["role"] != "system":
            default_system = Message(role="system", content=self.DEFAULT_SYSTEM_PROMPT)
            return [default_system] + list(messages)
        return messages

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        """
        Render a message. For assistant messages, is_last controls whether thinking is preserved
        (True) or stripped to empty <think></think> (False).
        """
        assert isinstance(message["content"], str), (
            "KimiK2Renderer only supports message with string content"
        )
        role = message["role"]
        role_name = message.get("name", role)

        # Build role token based on role type
        if role == "user":
            ob_str = f"<|im_user|>{role_name}<|im_middle|>"
        elif role == "assistant":
            ob_str = f"<|im_assistant|>{role_name}<|im_middle|>"
        elif role == "system":
            ob_str = f"<|im_system|>{role_name}<|im_middle|>"
        elif role == "tool":
            ob_str = f"<|im_system|>{role_name}<|im_middle|>"
            # Tool responses have special formatting
            tool_call_id = message.get("tool_call_id", "")
            ob_str += f"## Return of {tool_call_id}\n"
        else:
            ob_str = f"<|im_system|>{role_name}<|im_middle|>"

        # Build action content
        ac_str = ""
        if role == "assistant":
            # For the last assistant message (is_last=True), preserve thinking; otherwise use empty think block
            thinking = message.get("thinking", "")
            if is_last and thinking:
                ac_str = f"<think>{thinking}</think>"
            else:
                ac_str = "<think></think>"
            ac_str += message["content"]

            # Handle tool calls
            if "tool_calls" in message and message["tool_calls"]:
                ac_str += "<|tool_calls_section_begin|>"
                for tool_call in message["tool_calls"]:
                    tool_id = tool_call.id or ""
                    args = tool_call.function.arguments
                    ac_str += f"<|tool_call_begin|>{tool_id}<|tool_call_argument_begin|>{args}<|tool_call_end|>"
                ac_str += "<|tool_calls_section_end|>"
        else:
            ac_str = message["content"]

        ac_str += "<|im_end|>"

        prefix = tinker.types.EncodedTextChunk(tokens=self.tokenizer.encode(ob_str))
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(tokens=self.tokenizer.encode(ac_str))
        ]
        return RenderedMessage(prefix=prefix, content=content)

    def build_generation_prompt(
        self, messages: list[Message], role: Role = "assistant", prefill: str | None = None
    ) -> tinker.ModelInput:
        messages = self._ensure_system_message(messages)
        chunks: list[tinker.types.ModelInputChunk] = []

        for idx, message in enumerate(messages):
            # For generation prompt, no message is "last assistant" since we're generating new response
            rendered_message = self.render_message(idx, message, is_last=False)
            ob_chunk = rendered_message.get("prefix")
            action_chunks = rendered_message["content"]
            if ob_chunk:
                chunks.append(ob_chunk)
            chunks.extend([x for x in action_chunks if x])

        # Add generation prompt for new assistant message
        gen_prompt = f"<|im_assistant|>{role}<|im_middle|>"
        chunks.append(tinker.types.EncodedTextChunk(tokens=self.tokenizer.encode(gen_prompt)))
        if prefill:
            chunks.append(tinker.types.EncodedTextChunk(tokens=self.tokenizer.encode(prefill)))
        return tinker.ModelInput(chunks=chunks)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[tinker.ModelInput, torch.Tensor]:
        """
        Override to properly handle thinking preservation for the last assistant message.
        Also ensures default system message is prepended if none is present.
        """
        messages = self._ensure_system_message(messages)

        # Find last non-tool-call assistant message index
        last_assistant_idx = -1
        for idx in range(len(messages) - 1, -1, -1):
            if messages[idx]["role"] == "assistant" and "tool_calls" not in messages[idx]:
                last_assistant_idx = idx
                break

        model_input_chunks_weights: list[tuple[tinker.types.ModelInputChunk, float]] = []

        for idx, message in enumerate(messages):
            if train_on_what == TrainOnWhat.CUSTOMIZED:
                assert "trainable" in message, (
                    "When using CUSTOMIZED train_on_what, each message must have a trainable field"
                )
            else:
                assert "trainable" not in message, (
                    "When using non-CUSTOMIZED train_on_what, each message must not have a trainable field"
                )

            is_last_message = idx == len(messages) - 1
            is_assistant = message["role"] == "assistant"
            is_user_or_system = message["role"] in ["user", "system"]

            # For Kimi K2, preserve thinking only for last non-tool-call assistant
            is_last_assistant = idx >= last_assistant_idx and is_assistant
            rendered_message = self.render_message(idx, message, is_last=is_last_assistant)

            ob_part = rendered_message.get("prefix")
            action_parts = rendered_message.get("content")

            ob_weight = int(train_on_what == TrainOnWhat.ALL_TOKENS)
            if ob_part:
                model_input_chunks_weights += [(ob_part, ob_weight)]

            match train_on_what:
                case TrainOnWhat.LAST_ASSISTANT_MESSAGE:
                    action_has_weight = is_last_message and is_assistant
                case TrainOnWhat.ALL_ASSISTANT_MESSAGES:
                    action_has_weight = is_assistant
                case TrainOnWhat.ALL_MESSAGES:
                    action_has_weight = True
                case TrainOnWhat.ALL_TOKENS:
                    action_has_weight = True
                case TrainOnWhat.ALL_USER_AND_SYSTEM_MESSAGES:
                    action_has_weight = is_user_or_system
                case TrainOnWhat.CUSTOMIZED:
                    action_has_weight = message.get("trainable", False)
                case _:
                    raise ValueError(f"Unknown train_on_what: {train_on_what}")

            model_input_chunks_weights += [
                (action_part, int(action_has_weight)) for action_part in action_parts if action_part
            ]

        weights_data = [w for chunk, w in model_input_chunks_weights for _ in range(chunk.length)]
        weights_tensor = torch.tensor(weights_data)

        model_input_chunks = [chunk for chunk, _ in model_input_chunks_weights]
        return tinker.ModelInput(chunks=model_input_chunks), weights_tensor

    @property
    def _end_message_token(self) -> int:
        tokens = self.tokenizer.encode("<|im_end|>")
        assert len(tokens) == 1, f"Expected single token for <|im_end|>, got {len(tokens)}"
        return tokens[0]

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        assistant_message, parse_success = parse_response_for_stop_token(
            response, self.tokenizer, self._end_message_token
        )
        if not parse_success:
            return assistant_message, False

        content = assistant_message["content"]
        assert isinstance(content, str)

        # Extract thinking content if present
        think_match = re.search(r"<think>(.*?)</think>", content, re.DOTALL)
        if think_match:
            thinking = think_match.group(1)
            # Remove the think block from content
            content = content[think_match.end() :].lstrip()
            assistant_message["thinking"] = thinking
            assistant_message["content"] = content

        # Handle tool calls if present
        if "<|tool_calls_section_begin|>" in content:
            tool_section_match = re.search(
                r"<\|tool_calls_section_begin\|>(.*?)<\|tool_calls_section_end\|>",
                content,
                re.DOTALL,
            )
            if tool_section_match:
                tool_section = tool_section_match.group(1)
                tool_calls: list[ToolCall] = []
                unparsed_tool_calls: list[UnparsedToolCall] = []

                # Parse individual tool calls
                # Tool ID format: "functions.{func_name}:{idx}" e.g. "functions.get_weather:0"
                tool_call_pattern = r"<\|tool_call_begin\|>(.*?)<\|tool_call_argument_begin\|>(.*?)<\|tool_call_end\|>"
                for match in re.finditer(tool_call_pattern, tool_section, re.DOTALL):
                    raw_text = match.group(0)
                    tool_id = match.group(1).strip()
                    args_str = match.group(2).strip()

                    # Extract function name from tool_id (format: "functions.{name}:{idx}")
                    func_name = ""
                    if tool_id and "." in tool_id:
                        # Split on first dot to get "functions" prefix and the rest
                        parts = tool_id.split(".", 1)
                        if len(parts) == 2:
                            # Split on colon to separate name from index
                            name_part = parts[1].split(":")[0] if ":" in parts[1] else parts[1]
                            func_name = name_part

                    # Try to parse as JSON to validate, but store as string
                    try:
                        json.loads(args_str)
                        tool_calls.append(
                            ToolCall(
                                function=ToolCall.FunctionBody(name=func_name, arguments=args_str),
                                id=tool_id if tool_id else None,
                            )
                        )
                    except json.JSONDecodeError as e:
                        unparsed_tool_calls.append(
                            UnparsedToolCall(raw_text=raw_text, error=f"Invalid JSON: {e}")
                        )

                if tool_calls:
                    assistant_message["tool_calls"] = tool_calls
                if unparsed_tool_calls:
                    assistant_message["unparsed_tool_calls"] = unparsed_tool_calls

                # Remove tool section from content (both parsed and unparsed)
                if tool_calls or unparsed_tool_calls:
                    content = content[: content.find("<|tool_calls_section_begin|>")]
                    assistant_message["content"] = content

        return assistant_message, True

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        """Create system messages with Kimi K2 tool specifications.

        Per the HuggingFace chat template, Kimi K2 places the tool_declare message
        BEFORE the regular system message. If no system_prompt is provided, uses
        the default system prompt to match HuggingFace chat template behavior.

        Reference: https://huggingface.co/moonshotai/Kimi-K2-Thinking/blob/main/chat_template.jinja
        """
        messages: list[Message] = []

        # Tool declaration message comes first (per HF chat template)
        if tools:
            tools_json = json.dumps(tools, separators=(",", ":"))
            messages.append(Message(role="system", content=tools_json, name="tool_declare"))

        # Regular system message second (use default if none provided)
        actual_system_prompt = system_prompt if system_prompt else self.DEFAULT_SYSTEM_PROMPT
        messages.append(Message(role="system", content=actual_system_prompt))

        return messages


class GptOssRenderer(Renderer):
    """
    Format like this (no newlines between messages, last message should end with <|return|> but be replaced by <|end|> when continuing the convo):
        <|start|>system<|message|>You are ChatGPT...<|end|><|start|>user<|message|>How much is 1+1?<|end|><|start|>assistant<|channel|>final<|message|>2<|end|><|start|>
    TODO: support channels in input messages and tools
    """

    system_prompt = "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: {current_date}\n\nReasoning: {reasoning_effort}\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|>"
    use_system_prompt: bool = False
    reasoning_effort: str | None = None
    current_date: str | None = (
        None  # If use_system_prompt=True, will use the current date if this is None. Set this to a fixed date for deterministic system prompt.
    )

    def __init__(
        self,
        tokenizer: Tokenizer,
        use_system_prompt: bool = False,
        reasoning_effort: str | None = None,
        current_date: str | None = None,
    ):
        super().__init__(tokenizer)
        self.use_system_prompt = use_system_prompt
        self.reasoning_effort = reasoning_effort
        self.current_date = current_date
        assert use_system_prompt == (reasoning_effort is not None), (
            "Reasoning effort must be set iff using system prompt"
        )

    def render_message(self, idx: int, message: Message, is_last: bool = False) -> RenderedMessage:
        assert message.get("tool_calls") is None, "TODO: support tools in gpt-oss renderer"
        assert isinstance(message["content"], str), (
            "GptOssRenderer only supports message with string content"
        )
        # Observation (prompt) part
        # HF template maps "system" role to "developer" with special formatting
        role = message["role"]
        if role == "system":
            role = "developer"
        ob_str = f"<|start|>{role}"
        # Action part
        ac_str = ""
        if message["role"] == "assistant":
            # TODO: support commentary channel / tools

            # Assistant channels. See https://cookbook.openai.com/articles/openai-harmony
            thinking = message.get("thinking")
            message_content = message.get("content", "")
            assert isinstance(message_content, str), "GptOssRenderer only supports string content"

            # Analysis channel (CoT) - always included for last message to match HF template
            if is_last:
                thinking_content = thinking if thinking else ""
                ac_str += (
                    f"<|channel|>analysis<|message|>{thinking_content}<|end|><|start|>assistant"
                )

            # Final channel (Response Content)
            ac_str += f"<|channel|>final<|message|>{message_content}"
        elif message["role"] == "system":
            # HF wraps system content as developer instructions
            ac_str += f"<|message|># Instructions\n\n{message['content']}\n\n"
        else:
            assert message.get("thinking") is None, (
                "Thinking is only allowed for assistant messages"
            )
            ac_str += f"<|message|>{message['content']}"

        if not is_last:
            ac_str += "<|end|>"
        else:
            # <|return|> ends the last-message in harmony (but should be replaced by <|end|> when continuing the convo)
            ac_str += "<|return|>"

        prefix = tinker.types.EncodedTextChunk(
            tokens=self.tokenizer.encode(ob_str, add_special_tokens=False)
        )
        content: list[tinker.ModelInputChunk] = [
            tinker.types.EncodedTextChunk(
                tokens=self.tokenizer.encode(ac_str, add_special_tokens=False)
            )
        ]
        return RenderedMessage(prefix=prefix, content=content)

    def _build_system_prompt(self) -> str:
        current_date = (
            self.current_date
            if self.current_date is not None
            else datetime.now().strftime("%Y-%m-%d")
        )
        return self.system_prompt.format(
            current_date=current_date, reasoning_effort=self.reasoning_effort
        )

    @property
    def _bos_tokens(self) -> list[int]:
        tokens = []
        if self.use_system_prompt:
            tokens.extend(
                self.tokenizer.encode(self._build_system_prompt(), add_special_tokens=False)
            )
        return tokens

    @property
    def _return_token(self) -> int:
        res = self.tokenizer.encode("<|return|>", add_special_tokens=False)
        assert len(res) == 1, f"Expected single token for <|return|>, got {len(res)}"
        return res[0]

    def get_stop_sequences(self) -> list[int]:
        return [self._return_token]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        return parse_response_for_stop_token(response, self.tokenizer, self._return_token)

    def create_conversation_prefix_with_tools(
        self, tools: list[ToolSpec], system_prompt: str = ""
    ) -> list[Message]:
        raise NotImplementedError("GptOssRenderer does not support tool calling")


def get_renderer(
    name: str, tokenizer: Tokenizer, image_processor: ImageProcessor | None = None
) -> Renderer:
    if name == "role_colon":
        return RoleColonRenderer(tokenizer)
    elif name == "llama3":
        return Llama3Renderer(tokenizer)
    elif name == "qwen3":
        return Qwen3Renderer(tokenizer)
    elif name == "qwen3_vl":
        assert image_processor is not None, "qwen3_vl renderer requires an image_processor"
        return Qwen3VLRenderer(tokenizer, image_processor)
    elif name == "qwen3_vl_instruct":
        assert image_processor is not None, "qwen3_vl_instruct renderer requires an image_processor"
        return Qwen3VLInstructRenderer(tokenizer, image_processor)
    elif name == "qwen3_disable_thinking":
        return Qwen3DisableThinkingRenderer(tokenizer)
    elif name == "qwen3_instruct":
        return Qwen3InstructRenderer(tokenizer)
    elif name == "deepseekv3":
        return DeepSeekV3Renderer(tokenizer)
    elif name == "deepseekv3_disable_thinking":
        return DeepSeekV3DisableThinkingRenderer(tokenizer)
    elif name == "kimi_k2":
        return KimiK2Renderer(tokenizer)
    elif name == "gpt_oss_no_sysprompt":
        return GptOssRenderer(tokenizer, use_system_prompt=False)
    elif name == "gpt_oss_low_reasoning":
        return GptOssRenderer(tokenizer, use_system_prompt=True, reasoning_effort="low")
    elif name == "gpt_oss_medium_reasoning":
        return GptOssRenderer(tokenizer, use_system_prompt=True, reasoning_effort="medium")
    elif name == "gpt_oss_high_reasoning":
        return GptOssRenderer(tokenizer, use_system_prompt=True, reasoning_effort="high")
    else:
        raise ValueError(f"Unknown renderer: {name}")
