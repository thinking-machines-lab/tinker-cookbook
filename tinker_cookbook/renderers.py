"""
Use viz_sft_dataset to visualize the output of different renderers. E.g.,
    python -m tinker_cookbook.viz_sft_dataset dataset_name=wildchat_v0 renderer_name=role_colon
"""

import logging
from enum import StrEnum
from typing import Callable, TypedDict

import torch
from tinker_public import types

from tinker_cookbook.tokenizer_utils import Tokenizer

logger = logging.getLogger(__name__)


class Message(TypedDict):
    role: str
    content: str


class TrainOnWhat(StrEnum):
    LAST_ASSISTANT_MESSAGE = "last_assistant_message"
    ALL_ASSISTANT_MESSAGES = "all_assistant_messages"
    ALL_MESSAGES = "all_messages"
    ALL_TOKENS = "all_tokens"


class Renderer:
    def __init__(self, tokenizer: Tokenizer):
        self.tokenizer = tokenizer

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        raise NotImplementedError

    def build_generation_prompt(
        self, messages: list[Message], role: str = "assistant"
    ) -> types.ModelInput:
        raise NotImplementedError

    def get_stop_sequences(self) -> list[str] | list[int]:
        raise NotImplementedError

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        raise NotImplementedError


def tokens_weights_from_strings_weights(
    strings_weights: list[tuple[str, float]],
    tokenizer: Tokenizer,
) -> tuple[torch.Tensor, torch.Tensor]:
    strings, weights = zip(*strings_weights, strict=True)
    token_chunks = [tokenizer.encode(s, add_special_tokens=i == 0) for i, s in enumerate(strings)]
    weights = torch.cat(
        [torch.full((len(chunk),), w) for chunk, w in zip(token_chunks, weights, strict=True)]
    )
    tokens = torch.cat([torch.tensor(chunk) for chunk in token_chunks])
    assert tokens.dtype == torch.int64
    return tokens, weights


def build_supervised_example(
    start_tokens: list[int],
    render_message: Callable[[int, Message], tuple[list[int], list[int], list[int]]],
    messages: list[Message],
    train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Generates tokens and weights (for SFT) in the most standard way; by concatenating
    together tokens and weights for each message.

    Args:
        start_tokens: a list of tokens that are added at the beginning of the sequence.
        render_message: a function that takes an index and a message and returns a tuple of three lists of tokens:
            - ob_part: tokens for the observation part of the message
            - action_part: tokens for the action part of the message
            - action_tail: tokens that are generated by the assistant in this message, which are also
                part of the ob part of the next message. (Only relevant for some renderers, such as RoleColonRenderer)
        train_on_what: an enum that controls how the weights are assigned to the tokens.
            - TrainOnWhat.LAST_ASSISTANT_MESSAGE: only the last assistant message is used for training
            - TrainOnWhat.ALL_ASSISTANT_MESSAGES: all assistant messages are used for training
        messages: a list of messages to render.

    Returns:
        A tuple of two tensors:
            - tokens: a tensor of tokens
            - weights: a tensor of weights
    """
    tokens_weights = [(token, 0) for token in start_tokens]
    for idx, message in enumerate(messages[:-1]):
        ob_part, action_part, action_tail = render_message(idx, message)
        if train_on_what == TrainOnWhat.LAST_ASSISTANT_MESSAGE:
            tokens_weights.extend([(token, 0) for token in ob_part + action_part])
        elif train_on_what == TrainOnWhat.ALL_ASSISTANT_MESSAGES:
            tokens_weights += [(token, 0) for token in ob_part]
            # TODO: look at the previous action tail and its overlap with the current action part
            # and put weight of 1 on those tokens too.
            is_assistant = message["role"] == "assistant"
            tokens_weights += [(token, int(is_assistant)) for token in action_part]
        elif train_on_what == TrainOnWhat.ALL_MESSAGES:
            tokens_weights += [(token, 0) for token in ob_part]
            tokens_weights += [(token, 1) for token in action_part]
        elif train_on_what == TrainOnWhat.ALL_TOKENS:
            tokens_weights += [(token, 1) for token in ob_part + action_part]
        else:
            raise ValueError(f"Unknown train_on_what: {train_on_what}")
    ob_part, action_part, action_tail = render_message(len(messages) - 1, messages[-1])
    tokens_weights.extend([(token, 0) for token in ob_part])
    tokens_weights.extend([(token, 1) for token in action_part + action_tail])
    tokens, weights = zip(*tokens_weights, strict=True)
    return torch.tensor(tokens), torch.tensor(weights)


class RoleColonRenderer(Renderer):
    """
    format like this:
        User: <content>

        Assistant: <content>

    This is basically the format used by DeepSeek, and similar to the format used by Anthropic,
    except that they use "Human" instead of "User".
    """

    def _render_message(self, message: Message) -> tuple[list[int], list[int], list[int]]:
        ob_str = message["role"].capitalize() + ":"
        # Observation (prompt) part
        ac_str = " " + message["content"] + "\n\n"
        # Action part
        ac_tail_str = "User:" if message["role"] == "assistant" else "<UNUSED>"
        # Action part that's only included in the last message in SFT
        return (
            self.tokenizer.encode(ob_str, add_special_tokens=False),
            self.tokenizer.encode(ac_str, add_special_tokens=False),
            self.tokenizer.encode(ac_tail_str, add_special_tokens=False),
        )

    def build_generation_prompt(
        self, messages: list[Message], role: str = "assistant"
    ) -> types.ModelInput:
        tokens = []
        tokens.extend(self._bos_tokens)
        for message in messages:
            ob_part, action_part, action_tail = self._render_message(message)
            tokens.extend(ob_part)
            tokens.extend(action_part)
        new_partial_message = Message(role=role, content="")
        ob_part, _action_part, _action_tail = self._render_message(new_partial_message)
        tokens.extend(ob_part)
        return types.ModelInput.from_ints(tokens)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get tokens and weights for action corresponding to final message
        """
        return build_supervised_example(
            self._bos_tokens,
            lambda _idx, message: self._render_message(message),
            messages,
            train_on_what,
        )

    def get_stop_sequences(self) -> list[str]:
        return ["\n\nUser:"]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        str_response = self.tokenizer.decode(response)
        splitted = str_response.split("\n\nUser:")
        if len(splitted) == 1:
            logger.debug(f"Response is not a valid assistant response: {str_response}")
            return Message(role="assistant", content=str_response), False
        elif len(splitted) == 2:
            before, _after = splitted
            return Message(role="assistant", content=before.strip()), True
        else:
            raise ValueError(
                f"When parsing response, expected to split into 1 or 2 pieces using stop tokens, but got {len(splitted)}. "
                "You probably are using the wrong stop tokens when sampling"
            )

    @property
    def _bos_tokens(self) -> list[int]:
        bos_token_str = self.tokenizer.bos_token
        if bos_token_str is None:
            return []
        assert isinstance(bos_token_str, str)
        return self.tokenizer.encode(bos_token_str, add_special_tokens=False)


class Llama3Renderer(Renderer):
    """
    Format like this:
        <|begin_of_text|><|start_header_id|>system<|end_header_id|>

        You are a helpful AI assistant for travel tips and recommendations<|eot_id|><|start_header_id|>user<|end_header_id|>

        What can you help me with?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

    """

    def _render_message(self, message: Message) -> tuple[list[int], list[int], list[int]]:
        ob_str = f"<|start_header_id|>{message['role']}<|end_header_id|>\n\n"
        # Observation (prompt) part
        ac_str = f"{message['content']}<|eot_id|>"
        # Action part
        ac_tail_str = ""  # No action tail needed for Llama3 format
        # Action part that's only included in the last message in SFT
        return (
            self.tokenizer.encode(ob_str, add_special_tokens=False),
            self.tokenizer.encode(ac_str, add_special_tokens=False),
            self.tokenizer.encode(ac_tail_str, add_special_tokens=False),
        )

    def build_generation_prompt(
        self, messages: list[Message], role: str = "assistant"
    ) -> types.ModelInput:
        tokens = []
        tokens.extend(self._bos_tokens)
        for message in messages:
            ob_part, action_part, action_tail = self._render_message(message)
            tokens.extend(ob_part)
            tokens.extend(action_part)
        new_partial_message = Message(role=role, content="")
        ob_part, _action_part, _action_tail = self._render_message(new_partial_message)
        tokens.extend(ob_part)
        return types.ModelInput.from_ints(tokens)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get tokens and weights for action corresponding to final message
        """
        return build_supervised_example(
            self._bos_tokens,
            lambda _idx, message: self._render_message(message),
            messages,
            train_on_what,
        )

    @property
    def _bos_tokens(self) -> list[int]:
        return self.tokenizer.encode("<|begin_of_text|>", add_special_tokens=False)

    @property
    def _end_message_token(self) -> int:
        (token,) = self.tokenizer.encode("<|eot_id|>", add_special_tokens=False)
        return token

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        str_response = self.tokenizer.decode(response)
        splitted = str_response.split("<|eot_id|>")
        if len(splitted) == 1:
            logger.debug(f"Response is not a valid assistant response: {str_response}")
            return (Message(role="assistant", content=str_response), False)
        elif len(splitted) == 2:
            before, _after = splitted
            return (Message(role="assistant", content=before.strip()), True)
        else:
            raise ValueError(f"this shouldn't happen: {len(splitted)}")


class Qwen2p5Renderer(Renderer):
    """
    Format like this:
        <|im_start|>system
        You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
        <|im_start|>user
        What can you help me with?<|im_end|>
        <|im_start|>assistant
        I can help you with...<|im_end|>

    This renderer is from the Qwen 2.5 series, however, it works with Qwen 3 as well.
    It is just missing Qwen 3's functionality for removing thinking spans in multi-turn conversations.
    """

    def _render_message(self, idx: int, message: Message) -> tuple[list[int], list[int], list[int]]:
        maybe_newline = "\n" if idx > 0 else ""
        ob_str = f"{maybe_newline}<|im_start|>{message['role']}\n"
        # Observation (prompt) part
        ac_str = f"{message['content']}<|im_end|>"
        # Action part
        ac_tail_str = ""  # No action tail needed for Qwen format
        # Action part that's only included in the last message in SFT
        return (
            self.tokenizer.encode(ob_str, add_special_tokens=False),
            self.tokenizer.encode(ac_str, add_special_tokens=False),
            self.tokenizer.encode(ac_tail_str, add_special_tokens=False),
        )

    def _get_system_message(self, messages: list[Message]) -> Message:
        """Get system message, using default if none provided."""
        return Message(
            role="system",
            content="You are a helpful assistant.",
        )

    def build_generation_prompt(
        self, messages: list[Message], role: str = "assistant"
    ) -> types.ModelInput:
        tokens = []  # No BOS token for Qwen
        for idx, message in enumerate(messages):
            ob_part, action_part, _ = self._render_message(idx, message)
            tokens.extend(ob_part)
            tokens.extend(action_part)
        # Add generation prompt
        new_partial_message = Message(role=role, content="")
        ob_part, _, _ = self._render_message(len(messages), new_partial_message)
        tokens.extend(ob_part)

        return types.ModelInput.from_ints(tokens)

    def build_supervised_example(
        self,
        messages: list[Message],
        train_on_what: TrainOnWhat = TrainOnWhat.LAST_ASSISTANT_MESSAGE,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        """
        Get tokens and weights for action corresponding to final message.
        """
        system_msg = self._get_system_message(messages)
        messages_with_system = [system_msg] + messages
        return build_supervised_example(
            [], self._render_message, messages_with_system, train_on_what
        )

    @property
    def _end_message_token(self) -> int:
        tokens = self.tokenizer.encode("<|im_end|>", add_special_tokens=False)
        assert len(tokens) == 1, f"Expected single token for <|im_end|>, got {len(tokens)}"
        return tokens[0]

    def get_stop_sequences(self) -> list[int]:
        return [self._end_message_token]

    def parse_response(self, response: list[int]) -> tuple[Message, bool]:
        str_response = self.tokenizer.decode(response)
        splitted = str_response.split("<|im_end|>")
        if len(splitted) == 1:
            logger.debug(f"Response is not a valid assistant response: {str_response}")
            return Message(role="assistant", content=str_response), False
        elif len(splitted) == 2:
            before, _after = splitted
            return Message(role="assistant", content=before.strip()), True
        else:
            raise ValueError(
                f"When parsing response, expected to split into 1 or 2 pieces using stop tokens, but got {len(splitted)}. "
                "You probably are using the wrong stop tokens when sampling"
            )


def get_renderer(name: str, tokenizer: Tokenizer) -> Renderer:
    if name == "role_colon":
        return RoleColonRenderer(tokenizer)
    elif name == "llama3":
        return Llama3Renderer(tokenizer)
    elif name == "qwen2p5":
        return Qwen2p5Renderer(tokenizer)
    else:
        raise ValueError(f"Unknown renderer: {name}")
