"""
Example:
    python -m tinker_cookbook.run_inspect_evals model_path=tinker://fabfd72a-1451-4da4-8edf-7f32ae31bcfc/sampler_weights/checkpoint_final tasks=paws renderer_name=role_colon model_name=meta-llama/Llama-3.1-8B
"""

import asyncio
import logging
import os
import time
from typing import Sequence

import chz
import tinker
from inspect_ai import eval_async
from inspect_ai.model import ChatCompletionChoice as InspectAIModelOutputChoice
from inspect_ai.model import ChatMessage as InspectAIChatMessage
from inspect_ai.model import ChatMessageAssistant as InspectAIChatMessageAssistant
from inspect_ai.model import Content, modelapi
from inspect_ai.model import GenerateConfig as InspectAIGenerateConfig
from inspect_ai.model import Model as InspectAIModel
from inspect_ai.model import ModelAPI as InspectAIModelAPI
from inspect_ai.model import ModelOutput as InspectAIModelOutput
from inspect_ai.model import ModelUsage as InspectAIModelUsage
from inspect_ai.tool import ToolChoice as InspectAIToolChoice
from inspect_ai.tool import ToolInfo as InspectAIToolInfo
from termcolor import colored
from tinker import types

from tinker_cookbook import renderers
from tinker_cookbook.tokenizer_utils import get_tokenizer

logger = logging.getLogger(__name__)


def get_model_usage(
    tokenized_prompt: Sequence[int], responses: Sequence[types.SampledSequence]
) -> InspectAIModelUsage:
    """
    Given a tokenized prompt and a list of responses, return the number of tokens used/generated by the model.
    """
    num_input_tokens = len(tokenized_prompt)
    num_output_tokens = sum(len(r.tokens) for r in responses)
    total_tokens = num_input_tokens + num_output_tokens
    usage = InspectAIModelUsage(
        input_tokens=num_input_tokens, output_tokens=num_output_tokens, total_tokens=total_tokens
    )
    return usage


def convert_inspect_messages(messages: list[InspectAIChatMessage]) -> list[renderers.Message]:
    def assert_string(content: str | list[Content]) -> str:
        if isinstance(content, str):
            return content
        else:
            raise ValueError(f"Invalid content: {content}")

    return [
        renderers.Message(role=m.role, content=assert_string(m.content).strip()) for m in messages
    ]


@modelapi(name="tinker-sampling")
class InspectAPIFromTinkerSampling(InspectAIModelAPI):
    def __init__(
        self,
        renderer_name: str,
        model_name: str,
        model_path: str | None = None,
        base_url: str | None = None,
        api_key: str | None = None,
        api_key_vars: list[str] = [],
        config: InspectAIGenerateConfig = InspectAIGenerateConfig(),
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=api_key_vars,
            config=config,
        )
        service_client = tinker.ServiceClient(api_key=api_key)
        self.sampling_client = service_client.create_sampling_client(model_path=model_path)
        tokenizer = get_tokenizer(model_name)
        self.renderer = renderers.get_renderer(name=renderer_name, tokenizer=tokenizer)

    async def generate(
        self,
        input: list[InspectAIChatMessage],
        tools: list[InspectAIToolInfo],
        tool_choice: InspectAIToolChoice,
        config: InspectAIGenerateConfig,
    ) -> InspectAIModelOutput:
        """
        The main interface that needs to be implemented to test a new model.
        """
        convo = convert_inspect_messages(input)
        prompt = self.renderer.build_generation_prompt(convo)
        num_responses = 1 if config.num_choices is None else config.num_choices
        sampling_params = types.SamplingParams(
            temperature=config.temperature if config.temperature is not None else 1.0,
            max_tokens=config.max_tokens or 128,  # XXX
            stop=self.renderer.get_stop_sequences(),
            top_p=config.top_p if config.top_p is not None else 1.0,
            top_k=config.top_k if config.top_k is not None else -1,
        )

        start_time = time.time()
        assert num_responses == 1
        sample_result = await self.sampling_client.sample_async(
            prompt=prompt, sampling_params=sampling_params, num_samples=num_responses
        )
        sampled_token_sequences = sample_result.sequences
        print(colored(self.renderer.tokenizer.decode(prompt.to_ints()), "green"), end="")
        print(colored(self.renderer.tokenizer.decode(sampled_token_sequences[0].tokens), "red"))
        end_time = time.time()

        parsed_responses = [
            self.renderer.parse_response(r.tokens)[0] for r in sampled_token_sequences
        ]
        responses_text = [r["content"] for r in parsed_responses]
        all_choices = [
            InspectAIModelOutputChoice(
                message=InspectAIChatMessageAssistant(content=r, model=self.model_name),
                stop_reason="stop",
            )
            for r in responses_text
        ]
        usage = get_model_usage(prompt.to_ints(), sampled_token_sequences)

        return InspectAIModelOutput(
            model=self.model_name, choices=all_choices, time=end_time - start_time, usage=usage
        )


@chz.chz
class Config:
    renderer_name: str
    model_name: str  # TODO: shouldn't be required, once we can get it from the model_path
    model_path: str | None = None
    tasks: str  # comma separated list of tasks


async def main(config: Config):
    logging.basicConfig(level=logging.INFO)
    api = InspectAPIFromTinkerSampling(
        renderer_name=config.renderer_name,  # pyright: ignore[reportCallIssue]
        model_path=config.model_path,  # pyright: ignore[reportCallIssue]
        model_name=config.model_name,  # ^^^ not sure why pyright is unhappy about those
        api_key="asdf",  # FIXME
    )
    model = InspectAIModel(api=api, config=InspectAIGenerateConfig(temperature=1.0))

    os.environ["INSPECT_DISPLAY"] = "none"
    await eval_async(
        tasks=[f"inspect_evals/{t}" for t in config.tasks.split(",")],
        model=[model],
        limit=100,
        debug_errors=True,
        log_dir=os.path.expanduser("~/inspect-logs"),
        max_connections=100,
        log_level="INFO",
    )


if __name__ == "__main__":
    asyncio.run(chz.nested_entrypoint(main))
