Check out docs/rendering.mdx for more details on the rendering system, and docs/rl/sequence-extension.mdx for more details on the sequence extension property.

Some tips for development of renderers:

- As it's hard to audit all the parsing and prompt-building logic, focus on writing tests for properties that these functions should have:

    - Exact correspondence between `build_generation_prompt` and `apply_chat_template` from HuggingFace transformers
    - Correspondence between `build_supervised_example` and `build_generation_prompt`: if you you turn the conversation into a supervised example (tokens and weights), and split the tokens into observation and action, then the observation tokens should match the tokens that would be generated by `build_generation_prompt` for the same conversation prefix (all but the last assistant message).
    - Correspondence between `build_supervised_example` and `parse_response`: if you turn the conversation into a supervised example as described above, and parse the action tokens back into a message, you should get the same final assistant message back that you started with.
    - Sequence extension property: for applicable models, given a series of message user1, assistant1, user2, assistant2, ..., the observation from user1 should be a prefix of the observation from user2, and so on.

- For LLM assisted development, do some web research to put together specification of the token-level formatting conventions:

    - Do a local checkout of vLLM or SGLang and tell the LLM to use it for reference for determining token formats
    - Tell a web research agent to look up the token-level formatting conventions for the model in question (using developer docs, tech reports, chat templates, and other sources), and compile a markdown-formatted report that describes the rendering and parsing behavior.
